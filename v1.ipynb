{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing modules and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb                                                           # model library\n",
    "import pandas as pd                                                             # dataset processing\n",
    "import numpy as np                                                              # dataset filtering\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder                                # sklearn's preprocessing module\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV              # sklearn's model selection module\n",
    "from sklearn.feature_selection import SelectFromModel                           # sklearn's feature selection module\n",
    "from sklearn.metrics import classification_report                               # sklearn's metrics module\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC                                      # SMOTENC module from imbalanecd-learning package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "data = pd.read_csv(\"data/bank-additional-full.csv\", sep=\";\", index_col=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_ec = OrdinalEncoder() # defining an instance of the ordinal encoder\n",
    "\n",
    "## Applying a bunch of basic dataframe transformations using pd and np\n",
    "\n",
    "data['education'] = data['education'].replace(to_replace=['basic.4y','basic.6y','basic.9y'], value = 'basic')\n",
    "data['education'] = ordinal_ec.fit_transform(data[['education']])\n",
    "\n",
    "data['pdays'] = np.where(data['pdays'] != 999, 1, 0)\n",
    "\n",
    "data['y'] = data['y'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "data.drop('duration', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train-test splitting and class imbalance using SMOTE-NC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step is necessary since xgboost can't handle features that are of type `object`, they must be converted to `category` first or xgboost will error out\n",
    "\n",
    "object_columns = data.select_dtypes(include=['object']).columns # filters for `object` columns\n",
    "\n",
    "data[object_columns] = data[object_columns].astype('category') # transforms `object` columns to `category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset's target class is imbalanced (as is the case with most datasets really) so we need to balance it using SMOTE.\n",
    "# but since we have categorical variables in the data, we need to use SMOTE-NC (essentially an extension to SMOTE that supports categorical features)\n",
    "\n",
    "# categorical columns, in hindsight this could have also been done by filtering using dtypes like the code cell above but wtv for now\n",
    "cat_cols_smote_nc = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "# isolating the target class (y) from the features (X)\n",
    "X = data.loc[:, data.columns != 'y']\n",
    "y = data['y']\n",
    "\n",
    "# using sklearn's test_train_split() method to split the data into 4 components\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                test_size = 0.2,\n",
    "                                                random_state = 0)\n",
    "\n",
    "# applying SMOTE-NC algorithm on data to balance it. SMOTE-NC requires specifying which columns are categorical (hence why `cat_cols_smote_nc` exists)\n",
    "smotenc = SMOTENC(cat_cols_smote_nc,random_state = 0)\n",
    "X_train, y_train = smotenc.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model fitting and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.9s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   3.0s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   2.0s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.9s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.9s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   3.0s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   3.0s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   3.0s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.3s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.3s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.2s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.2s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=error, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.6s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   3.0s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.6s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=error, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.9s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.9s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=50; total time=   1.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.3s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.05, max_depth=10, n_estimators=100; total time=   3.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=50; total time=   1.7s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.3s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.3s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.2s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.0s\n",
      "[CV] END eval_metric=auc, learning_rate=0.1, max_depth=10, n_estimators=100; total time=   3.2s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=50; total time=   1.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.6s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=8, n_estimators=100; total time=   2.8s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=50; total time=   1.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   2.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   2.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   2.4s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   2.5s\n",
      "[CV] END eval_metric=auc, learning_rate=0.3, max_depth=10, n_estimators=100; total time=   2.4s\n"
     ]
    }
   ],
   "source": [
    "# creating the baseline xgboost model\n",
    "clf = xgb.XGBClassifier(enable_categorical=True, tree_method='approx')\n",
    "\n",
    "# defining the parameters that will be used for hyperparameter tuning (this can be very computationally expensive so select carefully!)\n",
    "params_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [8, 10],\n",
    "    'learning_rate': [0.05, 0.1, 0.3],\n",
    "    'eval_metric': ['error', 'auc'],\n",
    "}\n",
    "\n",
    "\n",
    "# creating and fitting instance of grid search on balanced dataset.\n",
    "# `cv` parameter is cross-validation (how many subsets of the training data will be sampled to measure the model on unseen data)\n",
    "# `verbose` set to 2 is nice because it tells you how many combinations GridSearch will iterate over and the parameters, score, \n",
    "# and computation time of every 'fit'. can be removed altogether tho, makes no difference \n",
    "# `error_score` always leave that to 'raise', will prevent wasting lots of time by directly flagging errors and exiting out of GridSearch for debugging\n",
    "grid_search = GridSearchCV(clf, params_grid, cv=5, n_jobs=-1, verbose=2, error_score='raise')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# grid search objects have a few attributes, .best_params_ pull the parameters that were fitted\n",
    "# .best_estimator_ is the model fit that had the highest evaluation score\n",
    "grid_best_params = grid_search.best_params_\n",
    "grid_best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature selection and final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82      7319\n",
      "           1       0.24      0.70      0.36       919\n",
      "\n",
      "    accuracy                           0.72      8238\n",
      "   macro avg       0.59      0.71      0.59      8238\n",
      "weighted avg       0.87      0.72      0.77      8238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SelectFromModel is a class of sklearn that lets you select features after a model has been fitted based on \n",
    "# how the model itself ranks the features. \n",
    "\n",
    "feature_selector = SelectFromModel(grid_best_model, prefit=True)\n",
    "feature_selector.fit(X_train, X_test)\n",
    "\n",
    "# get the indices of the selected features (essentially the number of the column on a 0-based index)\n",
    "selected_feature_indices = feature_selector.get_support(indices=True)\n",
    "\n",
    "# pull the corresponding column names of the selected indices\n",
    "selected_feature_names = X_train.columns[selected_feature_indices]\n",
    "\n",
    "# update the X_ train+test subsets to reflect the selected features\n",
    "X_train_selected = feature_selector.transform(X_train)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "# still using the best estimator and parameters from gridsearch, fit it with the selected features\n",
    "# this should technically give the best model possible if done right.\n",
    "final_model = grid_best_model.set_params(**grid_best_params)\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# classification_report is a cool function that generates a group of evaluation metrics for the model\n",
    "# instead of having to manually compute evaluation metrics seperately then consolidate them together! \n",
    "y_pred = final_model.predict(X_test_selected)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
